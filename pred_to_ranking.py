# Copyright (c) Microsoft Corporation.
# Copyright (c) 2021 HongChien Yu
# Licensed under the MIT license.

import csv
import os
import json
import numpy as np
import argparse

BM25_PIDS = "./data/top7/bm25_test/pids.tsv"
CRM_PIDS = "./data/top7/crm_test/pids.tsv"


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--prefix", help="Common prefix of the file names.")
    parser.add_argument("--initial_ranker",  choices=["bm25", "crm"])
    parser.add_argument("--trec_eval_path",  help="Path to the trec_eval software.")
    parser.add_argument("--gold_path", help="Path to TREC 2019 dev set gold standard relevance judgements.")
    return parser.parse_args()


def get_pids(pids_file):
    """
    Gets the pids of the top 1K retrievals.
    :param pids_file: Path to the tsv file mapping qid to pid
    :return: pids[qid] is the pids of the top 1K initial retrieval corresponding to qid
    """
    pids = dict()
    with open(pids_file, "r") as f:
        reader = csv.reader(f, delimiter="\t", quoting=csv.QUOTE_NONE)
        for row in reader:
            qid, pid = row[:2]
            pids[qid] = pids.get(qid, []) + [pid]
    return pids


def teout_to_csv(prefix):
    """
    Outputs the MAP and NDCG performances at different reranking depths to a .csv file.
    :param prefix: the prefix of the output path
    """
    metrics = ["map_cut_10", "map_cut_100", "map_cut_1000", "ndcg_cut_10"]
    metric_set = set(metrics)
    with open(f"{prefix}.csv", "w") as fout:
        writer = csv.writer(fout, delimiter="\t")
        writer.writerow(["depth"] + metrics)
        for depth in [20, 50, 100, 250, 500, 1000]:
            with open(f"{prefix}_newrerank{depth}.teout", "r") as fin:
                scores = dict()
                for l in fin:
                    metric, qry, score = l.split()
                    if metric in metric_set and qry == "all":
                        scores[metric] = float(score)
                writer.writerow([str(depth)] + [scores[metric] for metric in metrics])


def dict_str_to_float(d):
    """
    Destructively turns a Dict[str, List[str]] into Dict[str, List[float]]
    :param d: the dictionary to be converted
    """
    for qid, score_list in d.items():
        float_list = [float(score) for score in score_list]
        d[qid] = float_list


def json_to_tein(args, in_path, out_path, pids_file, rerank_depth=1000, total_num=1000):
    """
    Turns the model inference output .json file into a .tein file (the input to the trec_eval software)
    :param args: the parsed arguments
    :param in_path: the .out file generated by the model inference code
    :param out_path: the path to the output .tein file (the input to the trec_eval software)
    :param pids_file: Path to the tsv file mapping qid to pid
    :param rerank_depth: reranking depth
    :param total_num: number of initially retrieved documents per query
    :return:
    """
    pids = get_pids(pids_file)
    with open(in_path, "r") as fin, open(out_path, "w") as fout:
        writer = csv.writer(fout, delimiter="\t", quoting=csv.QUOTE_NONE)
        pred_dict = json.load(fin)
        dict_str_to_float(pred_dict)
        for qid, pred_list in pred_dict.items():
            to_rerank = [-score for score in pred_list[:rerank_depth]]
            sorted_idx = list(np.argsort(to_rerank))
            qry_pids = pids[qid]
            for rank, idx in enumerate(sorted_idx):
                rank = rank + 1
                pid = qry_pids[idx]
                # use -rank since the actual score is not relevant for the metrics
                writer.writerow([qid, "Q0", pid, rank, -rank, "trec"])
            for rank in range(rerank_depth, total_num):
                if rank >= len(pred_list):
                    break
                pid = qry_pids[rank]
                writer.writerow([qid, "Q0", pid, rank+1, -rank, "trec"])


def run_trec_eval(prefix, args):
    """
    Executes a system call to run the bash script, which produces a trec_eval output file (.teout)
    :param prefix: the prefix of the output path
    :param args: parsed arguments
    """
    script = f"{args.trec_eval_path} -q -c -M1000 -m all_trec {args.gold_path} {prefix}.tein > {prefix}.teout"
    os.system(script)


def test_trec(args):
    """
    Runs trec_eval at different reranking depths, generating performance summary .csv file
    :param args:
    :return:
    """
    if args.initial_ranker == "bm25":
        pids_file = BM25_PIDS
    elif args.initial_ranker == "crm":
        pids_file = CRM_PIDS
    prefix = args.prefix
    in_path = f"{prefix}.out"
    for depth in [20, 50, 100, 250, 500, 1000]:
        out_path = f"{prefix}_newrerank{depth}.tein"
        json_to_tein(args, in_path, out_path, pids_file=pids_file, rerank_depth=depth)
        run_trec_eval(f"{prefix}_newrerank{depth}", args)
    teout_to_csv(prefix)


def main():
    args = parse_args()
    test_trec(args)


if __name__ == "__main__":
    main()